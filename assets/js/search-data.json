{
  
    
        "post0": {
            "title": "Understanding Captum with CIFAR",
            "content": "Captum is an open-source extensible library for model interpretability and explainability built on PyTorch. . . . With the recent advancements in the Machine Learning there&#39;s a need to understand the working of the model in order to improve its performance and construct an explaination of the features learned, it is an active field of research as well as area of focus for practical applications across industries working with machine learning. Also, as the model complexity increases consequently we observe a lack of transaparency which might introduce biasness in the system. . Model Interpretability can also be used to provide explaination to questions that concern data ethicality, for example searching &quot;beautiful skin&quot; on google, results in &quot;White-skinned Women&quot;, if we were to train our model on data extracted from search enigines like google, bing, etc. our model is likely to associate beauty with white skin, which can be explained with methods like Integrated Gradients, DeepLift, etc. To study further about Data Ethics and biases in the datasets, click [here] (https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb) . This is a code-first tutorial with fastai2(will be officially updated to fastai in July 2020), since fastai is built on PyTorch so I have defined a custom pipeline and learner in fastai and used captum for visualisation of features. . from utils import * from fastai.vision.all import * . . Note: It&#8217;s important to set a random seed manually in order to have uniformity, and also it&#8217;s convenient to obtain the same set of random numbers in the form of an array or a tensor. . np.random.seed(42) torch.manual_seed(42) . &lt;torch._C.Generator at 0x111a9b210&gt; . path = untar_data(URLs.CIFAR) . !tree -d {path} . /Users/jay.0tist/.fastai/data/cifar10 ├── test │   ├── airplane │   ├── automobile │   ├── bird │   ├── cat │   ├── deer │   ├── dog │   ├── frog │   ├── horse │   ├── ship │   └── truck └── train ├── airplane ├── automobile ├── bird ├── cat ├── deer ├── dog ├── frog ├── horse ├── ship └── truck 22 directories . classes = L(file.parent.stem for file in files).unique() classes . (#10) [&#39;cat&#39;,&#39;dog&#39;,&#39;truck&#39;,&#39;bird&#39;,&#39;airplane&#39;,&#39;ship&#39;,&#39;frog&#39;,&#39;horse&#39;,&#39;deer&#39;,&#39;automobile&#39;] . from matplotlib import pyplot as plt def imshow(img, save=False, name=None): plt.imshow(img) if save: plt.savefig(name) plt.show() . import torch.nn as nn import torch.nn.functional as F . The model consists of two 2D-convolutional layers and 3 Linear Layers, each convolutional layer is followed by a MaxPool2d layer, the obtained actiavtion map from the last layer is stretched into a vector. The vector produced are then passed onto the linear layer to obtain the probabilities of the classes. . class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool1 = nn.MaxPool2d(2,2) self.pool2 = nn.MaxPool2d(2,2) self.conv2 = nn.Conv2d(6, 15, 5) self.fc1 = nn.Linear(15*4*4, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) self.relu1 = nn.ReLU() self.relu2 = nn.ReLU() self.relu3 = nn.ReLU() self.relu4 = nn.ReLU() def forward(self, x): x = self.pool1(self.relu1(self.conv1(x))) x = self.pool2(self.relu2(self.conv2(x))) x = x.view(-1, 15*4*4) x = self.relu3(self.fc1(x)) x = self.relu4(self.fc2(x)) x = self.fc3(x) return x net = Model() . Model( (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(6, 15, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=240, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) (relu1): ReLU() (relu2): ReLU() (relu3): ReLU() (relu4): ReLU() ) . datablock = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(28), batch_tfms=aug_transforms() ) . dls = datablock.dataloaders(path_train, bs=64) dls.train.show_batch(max_n=4, nrows=1) . ind = 4 img, label = next(iter(dls.train)) . print(label[ind]) imshow(np.transpose(img[ind].squeeze().cpu().detach().numpy(), (1,2,0))) . tensor(0) . learner = Learner(dls, net, loss_func=nn.CrossEntropyLoss(), metrics=[accuracy]) . model_path = Path(&#39;models/captum_with_cifar.pth&#39;) if(model_path.is_file()): learner = learner.load(&#39;captum_with_cifar&#39;) else: print(&quot;the model ain&#39;t available&quot;) . the model ain&#39;t available . learner.fit_one_cycle(30, 2e-4) . epoch train_loss valid_loss accuracy time . 0 | 2.303049 | 2.303556 | 0.125700 | 00:17 | . 1 | 2.285601 | 2.278230 | 0.174700 | 00:18 | . 2 | 2.093108 | 2.059948 | 0.230600 | 00:18 | . 3 | 1.981493 | 1.961067 | 0.276900 | 00:19 | . 4 | 1.890269 | 1.849878 | 0.314300 | 00:19 | . 5 | 1.807018 | 1.754402 | 0.355600 | 00:19 | . 6 | 1.738134 | 1.687229 | 0.378400 | 00:22 | . 7 | 1.703322 | 1.696606 | 0.386000 | 00:23 | . 8 | 1.625500 | 1.575260 | 0.419600 | 00:23 | . 9 | 1.603480 | 1.539367 | 0.434700 | 00:21 | . 10 | 1.570834 | 1.524960 | 0.443300 | 00:21 | . 11 | 1.562550 | 1.500113 | 0.454100 | 00:22 | . 12 | 1.529327 | 1.472827 | 0.467200 | 00:22 | . 13 | 1.510792 | 1.476959 | 0.465400 | 00:22 | . 14 | 1.482278 | 1.443329 | 0.475800 | 00:22 | . 15 | 1.500630 | 1.430311 | 0.484800 | 00:22 | . 16 | 1.464584 | 1.459092 | 0.474600 | 00:23 | . 17 | 1.457291 | 1.425829 | 0.482700 | 00:22 | . 18 | 1.455780 | 1.399747 | 0.493300 | 00:22 | . 19 | 1.452798 | 1.402900 | 0.493100 | 00:23 | . 20 | 1.471801 | 1.398465 | 0.496600 | 42:13 | . 21 | 1.416165 | 1.384726 | 0.503600 | 00:17 | . 22 | 1.427612 | 1.388004 | 0.498000 | 00:18 | . 23 | 1.418190 | 1.374975 | 0.504900 | 00:18 | . 24 | 1.415139 | 1.368006 | 0.507300 | 00:18 | . 25 | 1.426101 | 1.369255 | 0.505200 | 00:20 | . 26 | 1.403279 | 1.370924 | 0.504200 | 00:19 | . 27 | 1.436325 | 1.368910 | 0.506500 | 00:19 | . 28 | 1.399421 | 1.367629 | 0.506600 | 00:19 | . 29 | 1.391088 | 1.367739 | 0.505600 | 00:19 | . # !mkdir models . learner.save(&#39;captum_with_cifar&#39;) . Path(&#39;models/captum_with_cifar.pth&#39;) . from captum.attr import IntegratedGradients, Saliency from captum.attr import DeepLift, NoiseTunnel from captum.attr import visualization as viz . input = img[ind].unsqueeze(0) input.requires_grad = True . original_img = np.transpose(img[ind].squeeze().cpu().detach().numpy(), (1,2,0)) _ = viz.visualize_image_attr(None, original_img, method=&#39;original_image&#39;, title=&#39;Original Image&#39;) . saliency = Saliency(net) grads = saliency.attribute(input, target=label[ind].item()) grads = np.transpose(grads.squeeze().cpu().detach().numpy(), (1, 2, 0)) _ = viz.visualize_image_attr(grads, original_img, show_colorbar=True, method=&quot;blended_heat_map&quot;, sign=&quot;absolute_value&quot;, title=&quot;Overlayed gradient magnitude&quot;) . Saliency Maps are a way to visualise the features learned by the model, it essentially depicts the contribution of each pixel in the image towards the classification score of a particular class, to learn more about saliency map click here. . grads = torch.from_numpy(grads) grads, i = torch.max(grads.abs(), dim=2) . imshow(grads) . def attribute_image_features(algo, input, **kwargs): tensor_attributions = algo.attribute(input, target=label[ind].item(), **kwargs) return tensor_attributions . algo = IntegratedGradients(net) attribute, delta = attribute_image_features(algo, input, baselines=input*0, return_convergence_delta=True) attribute = np.transpose(attribute.squeeze().cpu().detach().numpy(), (1,2,0)) print(&#39;approximated delta : &#39;, delta) . approximated delta : tensor([0.0095], dtype=torch.float64) . Integrated Gradients . Before introducing integrated gradients we first need to understand the two fundamental axioms - sensitivity(and baseline value) and Implementation Variance . Sensitivity and Baseline Value . . Note: We have assumed that Gradients of F(x) with respect to variable x tells us the importance of x in F(x) . Sensitivity can be defined as the change in the output observed by changing a single variable in the function, keeping the rest of them constant, this change obtained is called the attribution of that variable, or simply one can say for a function: h&theta;(x, y)= &theta;0x + &theta;1y we assume a baseline of the variable x as x0 and we change x0 to x1 the change or difference obtained observed is : . h&theta;(x0, y) - h&theta;(x1, y) = &theta;0x0 + &theta;1y - &theta;0x1 - &theta;1y = &theta;0(x0 - x1) sensitivity = &theta;0(x0 - x1) Baseline value is defined as the value of the variable for which there is no contribution in the h&theta;(x ), that means the attribution due to the variable is equals to 0, from the same example we can say that when x = 0, the output is independent of x. h&theta;(0, y)= &theta;1y, so the baseline value for x = 0, in case of images as inputs, the baseline value can be black image or in case of numerical data, we assume the baseline value as the mode value . . Note: Attrbution in our case is simply the product of gradient of x with x or gradient of y with y, whichever is changing with respect to its baseline value . Implementation Variance . If two neural networks are defined on the same set of input and results in the same output, then the two neural networks are similar in their function. Attribution should be the same for both of the neural networks irrespective of the hidden layer implementation within the neural network. Implementation Variance can be verified with the help of the chain rule : . $$ frac{ partial f}{ partial h} cdot frac{ partial h}{ partial g} $$ . In the above equation, $f$ and $g$ are the ouput and the input functions respectively and $h$ is the hidden layer(implementation), as we know backpropagation is used to relay the error to the previous layers, similarly we can relay/propagate the importance to all the neurons in each layer . As we have understood Sensitivity, Baseline and Implementation invariance . Sensitivity check on Gradients . Consider a toy function: F(x) = (1 - ReLu(1 - x)), the attribution with respect to $ x $ can be expressed as $ frac{dF(x)}{dx} $, for points $ x geq 1 $ as show in the Fig. 1., $ F&#39;(x) = 0 $, in other words attribution for each $ x geq 1 $ will be 0, but we have defined that attribution will only be 0 at the baseline value. Hence, the sensitivty failed on the toy function, with that we can conclude that calculating derivatives isn&#39;t a viable option to evaluate attributions, as neural networks has ReLu() activation layers . The New way . we find the gradient along the path from the baseline value $x&#39;$ to $x$ and then we integrate all those gradients by averaging them. . $$ IntergratedGradients_i(x) equiv (x_i - x_i&#39;) times int_{ alpha = 0}^{1} frac{ partial F( alpha x + x&#39;(1 - alpha))}{ partial x_i} d alpha $$ . Does the New Method satisfies the sensitivity and Implementation variance? . Yes, It does. Let&#39;s understand this with the help of an example: Let&#39;s assume a, b be two values of the input variable $x$, a is the baseline value and b is the location at which we want our model to predict. The difference between prediction at both the points is the average of the integration of the gradients along the points from a to b. Implementation variance is also satisfied as the integrated gradients is based on gradients of the network and chain rule is applicable over gradients. . def relu(x): if(x &gt; 0): return x else: return 0 def toy_fn(x): return (1 - relu(1 - x)) . _ = viz.visualize_image_attr(attribute, original_img, show_colorbar=True, method=&quot;blended_heat_map&quot;, sign=&quot;all&quot;, title=&quot;Overlayed Integrated Gradients&quot;) . ig = IntegratedGradients(net) nt = NoiseTunnel(ig) attribute_nt = attribute_image_features(nt, input, baselines=input*0, nt_type=&#39;smoothgrad_sq&#39;, n_samples=100, stdevs=0.2) attribute_nt = np.transpose(attribute_nt.squeeze().cpu().detach().numpy(), (1,2,0)) _ = viz.visualize_image_attr(attribute_nt, original_img, show_colorbar=True, outlier_perc=10, method=&#39;blended_heat_map&#39;, sign=&quot;absolute_value&quot;, title=&quot;Overlayed Integrated Gradients n with smoothGrad square&quot;) . dl = DeepLift(net) attribute_dl = attribute_image_features(dl, input, baselines=input*0) attribute_dl = np.transpose(attribute_dl.squeeze().cpu().detach().numpy(), (1,2,0)) _ = viz.visualize_image_attr(attribute_dl, original_img, show_colorbar=True, method=&quot;blended_heat_map&quot;, sign=&quot;all&quot;, title=&quot;Overlayed DeepLift&quot;) . /Users/jay.0tist/.pyenv/versions/3.8.5/lib/python3.8/site-packages/captum/attr/_core/deep_lift.py:298: UserWarning: Setting forward, backward hooks and attributes on non-linear activations. The hooks and attributes will be removed after the attribution is finished warnings.warn( .",
            "url": "https://jayesh0vasudeva.github.io/Learning-Iteratively/2020/09/08/captum-tutorial.html",
            "relUrl": "/2020/09/08/captum-tutorial.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Iteration-0",
            "content": "My First Blog! :) . “Our intelligence is what makes us human, and AI is an extension of that quality.” — Yann LeCun, Professor NYU . Deep learning is really a powerful metaphor for learning about the world! . Deep Learning / Machine Learning are the latest buzz words that you will hear people talk about a lot, especially those who are affiliated to the tech industry and I’m one of those students who aspire to contribute in the field and grow along with it. I studied Competitive Programming and really enjoyed it. I have learned that it significantly improves programming skills and the understanding of algorithms. Yet, I found myself wanting to try something different and new, and came across one of youtube’s suggestions about advancing career options for the future in the software field. I became even more inquisitive about this field of study and soon realised that the applications of Deep Learning is limited to one’s imagination. As I progressed I started developing intuition around most of the optimised processes around us, which catered my brain and heightened my skepticism. Deep Learning is inspired by the functioning of human brain and how the neurons work, essentially we try to create equivalent mathematical models to imitate fundamental tasks that every human performs on a daily basis, for instance: differentiation between objects (apple &amp; orange), understanding texts etc. Now Learning, Deep Learning is an Iterative process and requires a lot of commitment to programming language one has chosen and all the iterations that you have been through so far, since it’s a Young field(with respect to its applications in the industry) and considerably very active for research, one needs to learn, practise and keep themselves updated. . How to learn from this Iterative Process? . By far Python is the most popular language as per the survey of 24,000 Data professionals by Kaggle, some of the few sources from where one can learn python: . official python 3 documentation | Corey schafer’s tutorial | Introduction to Computer Science and Programming using python | . Now for the Data Science / Machine Learning / Deep Learning there are tons of resources and courses available and I have explored a lot of them, for example: . CS229 - machine learning and statistical pattern recognition By Andrew Ng | CS231n - Computer vision By Fei Fei Li and associates | CS7015 - IIT Madras Deep learning | . The content of these courses is very well structured and is taught by the greatest minds. There was something unique in the way Prof. Jeremy Howard at USFCA teaches practical deep learning for coders, watched a couple of videos from his YouTube channel and found his way of teaching very comprehensible and not much mathematical. Not to mention, high-school mathematics is enough to get started for deep learning. He follows a top-down approach which involves coding along with devising simple equivalent mathematical equations. Hence, I decided to get myself enrolled in Fast.ai course v4 2020. Currently, I’m in the 2nd week of the course, trying to get some insights in the library fastai and fastai2 . . . What is Fastai and Why I favor it? . ‘fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: . A new type dispatch system for Python along with a semantic type hierarchy for tensors | A GPU-optimized computer vision library which can be extended in pure Python | An optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code | A novel 2-way callback system that can access any part of the data, model, or optimizer and change it at - any point during training | A new data block API …and much more.’ — source | . Besides the aforementioned points, fastai respects the read–eval–print loop (REPL) environment of the Jupyter Notebook, which can prove to be very handy at times. Most Importantly the layered API doesn’t only consist of a high-level and a low-level system but also a Mid-level system which helps to transit from the high-level, easy to use beginner API to low-level heavily featured API. There are more aspects to the the Library which I will discuss in the upcoming blogs. The course closely resembles the book “Deep Learning for coders with Fastai and Pytorch” by Jeremy Howard &amp; Sylvain gugger which will be available later this summer. . . About the Blog Posts Series . The motivation behind the bolg is highly self-centered, as it helps me keep track of the course and my development, following the advice of Rachel Thomas(co-founder fastai), check out her blog. Being on a lockdown rather proved to be helpful as it would be a tedious task to follow along with my university lectures. Will try to make best out of the opportunity and will update you guys on my upcoming posts soon! stay indoors, stay safe, take care :) . PS: Special mention to the amazing community of Fastai and their support on the forums, they are a constant help and will support you throughout your journey in the fastai, there are walkthroughs of the fastai code repository available on the youtube, which I will try to include whenever necessary. .",
            "url": "https://jayesh0vasudeva.github.io/Learning-Iteratively/markdown/2020/03/31/my-first-blog.html",
            "relUrl": "/markdown/2020/03/31/my-first-blog.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jayesh0vasudeva.github.io/Learning-Iteratively/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey ! How you doing? This is Jayesh Vasudeva . # Who am I? I’m an undergraduate student at Netaji Subhas University of technology(erstwhile Netaji Subhas Institue of technology). I am on a self-learning journey to absorb as much as I can about the Data science, Deep Learning and Software development on general. I spent most of my days reading reearch papers and related codes on GitHub,as a result I developed an interest in Research and Development. I’m always interested in taking on new projects and participating on online contests and events, besides that I’m an active member of IEEE Delhi Section and Computer Science Chapter chair at IEEE NSUT student chapter. We as a team, are constantly looking up for new open source projects and resources to develop and contribute in amazing projects. I love mathematics and Software and their intersection is beatiful. . What am I upto these days? . I’m currently mid-level api of fastai as a personal project, besides that I’m researching on various contact-tracing methods and predicting Hotspots/red-zone areas in India and also trying to develop a novel method to produce cancelable biometrics. I am actively contributing to fastai and NumCpp and looking for contributing in similar projects. . I’m always open to discussions and sharing resources to know more about Deep Learning and work on some interesting projects or maybe exchange ideas! Anyway, thank you for stopping by. Reach me Instagram: @jay.not.j Twitter: @VasudevaJayesh Wish to collaborate? reach me @ Gmail or LinkedIn . Also, check out my resume .",
          "url": "https://jayesh0vasudeva.github.io/Learning-Iteratively/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}